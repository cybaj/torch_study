{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "https://pytorch.org/docs/1.1.0/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.utils.data.DataLoader\n",
    "  \n",
    "Combines a dataset and a sampler,  \n",
    "  \n",
    "and provides **single- or multi-process iterators** over the dataset.  \n",
    "  \n",
    "(data size 만이 아니라)  \n",
    "Batch size 와 iterating worker number 은 데이터를 불러올 때, 필요한 메모리 사이즈를 결정한다.  \n",
    "*device에 대한 결정은 iterated 되어 나온 data에 직접 해야 한다.*\n",
    "  \n",
    "```  \n",
    "torch.utils.data.DataLoader(\n",
    "  dataset,  \n",
    "  batch_size=1,  \n",
    "  shuffle=False,  \n",
    "  sampler=None,  \n",
    "  batch_sampler=None,  \n",
    "  num_workers=0,  \n",
    "  collate_fn=<function default_collate>,  \n",
    "  pin_memory=False,  \n",
    "  drop_last=False,  \n",
    "  timeout=0,  \n",
    "  worker_init_fn=None)\n",
    "```\n",
    "parameters\n",
    "* **dataset (Dataset)** – dataset from which to load the data.\n",
    "  \n",
    "  \n",
    "* **batch_size** (int, optional) – how many samples per batch to load (default: 1).\n",
    "* **shuffle** (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n",
    "  \n",
    "  \n",
    "* sampler (Sampler, optional) – defines the strategy to draw samples from the dataset. If specified, shuffle must be False.\n",
    "* batch_sampler (Sampler, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.\n",
    "  \n",
    "  \n",
    "* **num_workers** (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "  \n",
    "  \n",
    "* collate_fn (callable, optional) – merges a list of samples to form a **mini-batch**.\n",
    "* pin_memory (bool, optional) – If True, the data loader will copy tensors into **CUDA pinned memory** before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type see the example below.\n",
    "  \n",
    "  \n",
    "* drop_last (bool, optional) – set to True to drop the **last incomplete batch**, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)\n",
    "  \n",
    "  \n",
    "* timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)\n",
    "* worker_init_fn (callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### num_workers != 0\n",
    "`DataLoader`를 위한 `iterator`가 얻어질 때마다 `worker process`가 만들어진다.  \n",
    "각 `worker process`에 `collate_fn`과 `worker_init_fn`가 전달되어지고, batch list 를 만들 때 (`collate_fn`), 그리고 seeding 과 data loading 사이에 (`worker_init_fn`) 호출된다.  \n",
    "  \n",
    "shuffle randomization 은 main process에 일어나고, iteration이 끝나면 모든 workers가 shutdown 된다.\n",
    "  \n",
    "worker 는 python multiprocessing 에 의존하고, 동작은 운영체제 마다 다를 수 있다. (유닉스와 윈도우는 다르다.)\n",
    "이 떄 *window compatible* 을 보증하기 위해 아래 2가지를 지켜야 한다.  \n",
    "* main script's code 를 거의 전부 `if __name__ == '__main__':` 블럭 안에 넣는다. worker 에 대한 로직이 다시 실행되지 않게 하기 위해서라고 함.\n",
    "* 단, `collate_fn`, `worker_init_fn` 또는 어떤 *custom dataset code* 를 top level def로, `__main__` 밖에서 정의해야 한다. 이렇게 해서 모든 worker 들이 접근할 수 있게 한다.\n",
    "  \n",
    "worker 각각이 각각의 random seed 를 갖는데, 다른 라이브러리에 사용되는 seed 중복될 수 있다. 이렇게 되어 각 시드가 같은 난수를 생성하게 될 때에 이를 피하고자 한다면, `worker_init_fn` 안에 `torch.initial_seed()`를 사용 할 수 있다.\n",
    "\n",
    "또 `spawn`이 사용되는 경우, (윈도우 환경에서는 `fork()` 대신에  `spawn()` 에서 사용됨)  \n",
    "lambda function 같은 unpickable object를 `worker_init_fn` 으로 사용할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataLoader in module torch.utils.data.dataloader:\n",
      "\n",
      "class DataLoader(builtins.object)\n",
      " |  DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=<function default_collate at 0x7f6b430c80d0>, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)\n",
      " |  \n",
      " |  Data loader. Combines a dataset and a sampler, and provides\n",
      " |  single- or multi-process iterators over the dataset.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler, optional): defines the strategy to draw samples from\n",
      " |          the dataset. If specified, ``shuffle`` must be False.\n",
      " |      batch_sampler (Sampler, optional): like sampler, but returns a batch of\n",
      " |          indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
      " |          :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. 0 means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy tensors\n",
      " |          into CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your ``collate_fn`` returns a batch that is a custom type\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |  \n",
      " |  .. note:: When ``num_workers != 0``, the corresponding worker processes are created each time\n",
      " |            iterator for the DataLoader is obtained (as in when you call\n",
      " |            ``enumerate(dataloader,0)``).\n",
      " |            At this point, the dataset, ``collate_fn`` and ``worker_init_fn`` are passed to each\n",
      " |            worker, where they are used to access and initialize data based on the indices\n",
      " |            queued up from the main process. This means that dataset access together with\n",
      " |            its internal IO, transforms and collation runs in the worker, while any\n",
      " |            shuffle randomization is done in the main process which guides loading by assigning\n",
      " |            indices to load. Workers are shut down once the end of the iteration is reached.\n",
      " |  \n",
      " |            Since workers rely on Python multiprocessing, worker launch behavior is different\n",
      " |            on Windows compared to Unix. On Unix fork() is used as the default\n",
      " |            muliprocessing start method, so child workers typically can access the dataset and\n",
      " |            Python argument functions directly through the cloned address space. On Windows, another\n",
      " |            interpreter is launched which runs your main script, followed by the internal\n",
      " |            worker function that receives the dataset, collate_fn and other arguments\n",
      " |            through Pickle serialization.\n",
      " |  \n",
      " |            This separate serialization means that you should take two steps to ensure you\n",
      " |            are compatible with Windows while using workers\n",
      " |            (this also works equally well on Unix):\n",
      " |  \n",
      " |            - Wrap most of you main script's code within ``if __name__ == '__main__':`` block,\n",
      " |              to make sure it doesn't run again (most likely generating error) when each worker\n",
      " |              process is launched. You can place your dataset and DataLoader instance creation\n",
      " |              logic here, as it doesn't need to be re-executed in workers.\n",
      " |            - Make sure that ``collate_fn``, ``worker_init_fn`` or any custom dataset code\n",
      " |              is declared as a top level def, outside of that ``__main__`` check. This ensures\n",
      " |              they are available in workers as well\n",
      " |              (this is needed since functions are pickled as references only, not bytecode).\n",
      " |  \n",
      " |            By default, each worker will have its PyTorch seed set to\n",
      " |            ``base_seed + worker_id``, where ``base_seed`` is a long generated\n",
      " |            by main process using its RNG. However, seeds for other libraies\n",
      " |            may be duplicated upon initializing workers (w.g., NumPy), causing\n",
      " |            each worker to return identical random numbers. (See\n",
      " |            :ref:`dataloader-workers-random-seed` section in FAQ.) You may\n",
      " |            use :func:`torch.initial_seed()` to access the PyTorch seed for\n",
      " |            each worker in :attr:`worker_init_fn`, and use it to set other\n",
      " |            seeds before data loading.\n",
      " |  \n",
      " |  .. warning:: If ``spawn`` start method is used, :attr:`worker_init_fn` cannot be an\n",
      " |               unpicklable object, e.g., a lambda function.\n",
      " |  \n",
      " |  The default memory pinning logic only recognizes Tensors and maps and iterables\n",
      " |  containg Tensors.  By default, if the pinning logic sees a batch that is a custom type\n",
      " |  (which will occur if you have a ``collate_fn`` that returns a custom batch type),\n",
      " |  or if each element of your batch is a custom type, the pinning logic will not\n",
      " |  recognize them, and it will return that batch (or those elements)\n",
      " |  without pinning the memory.  To enable memory pinning for custom batch or data types,\n",
      " |  define a ``pin_memory`` method on your custom type(s).\n",
      " |  \n",
      " |  Example::\n",
      " |  \n",
      " |      class SimpleCustomBatch:\n",
      " |          def __init__(self, data):\n",
      " |              transposed_data = list(zip(*data))\n",
      " |              self.inp = torch.stack(transposed_data[0], 0)\n",
      " |              self.tgt = torch.stack(transposed_data[1], 0)\n",
      " |  \n",
      " |          def pin_memory(self):\n",
      " |              self.inp = self.inp.pin_memory()\n",
      " |              self.tgt = self.tgt.pin_memory()\n",
      " |              return self\n",
      " |  \n",
      " |      def collate_wrapper(batch):\n",
      " |          return SimpleCustomBatch(batch)\n",
      " |  \n",
      " |      inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
      " |      tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
      " |      dataset = TensorDataset(inps, tgts)\n",
      " |  \n",
      " |      loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
      " |                          pin_memory=True)\n",
      " |  \n",
      " |      for batch_ndx, sample in enumerate(loader):\n",
      " |          print(sample.inp.is_pinned())\n",
      " |          print(sample.tgt.is_pinned())\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=<function default_collate at 0x7f6b430c80d0>, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "help(DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. using dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(epochs):\\n    for samples, labels in dataloader:\\n        samples, labels = samples.to(device), labels.to(device)\\n        optimizer.zero_grad()\\n        output = model(samples)\\n        loss = criterion(output, labels)\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item()\\n        scheduler.step()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cats = CatDogDataset(cat_files, train_dir, transform = data_transform)\n",
    "dogs = CatDogDataset(dog_files, train_dir, transform = data_transform)\n",
    "\n",
    "catdogs = ConcatDataset([cats, dogs])\n",
    "'''\n",
    "\n",
    "# dataloader = DataLoader(catdogs, batch_size = 6, shuffle=True, num_workers=0)\n",
    "\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    for samples, labels in dataloader:\n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(samples)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. memory pinning\n",
    "\n",
    "memory pinning 은 **cpu -> gpu data transfer** 시에 아주 유용하다.  \n",
    "https://mkblog.co.kr/2017/03/07/nvidia-gpu-pinned-host-memory-cuda/\n",
    "\n",
    "그런데 pytorch 에서는 `tensor` 에 대해서만 로직이 동작한다고 한다. 어떤 `batch`나 `custom data types` 을 위해서는 `custom type` 안에 `pin_memory()`를 정의해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCustomBatch:\n",
    "    def __init__(self, data):\n",
    "        transposed_data = list(zip(*data))\n",
    "        self.inp = torch.stack(transposed_data[0], 0)\n",
    "        self.tgt = torch.stack(transposed_data[1], 0)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.inp = self.inp.pin_memory()\n",
    "        self.tgt = self.tgt.pin_memory()\n",
    "        return self\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    return SimpleCustomBatch(batch)\n",
    "\n",
    "inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "dataset = TensorDataset(inps, tgts)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
    "                    pin_memory=True)\n",
    "\n",
    "for batch_ndx, sample in enumerate(loader):\n",
    "    print(sample.inp.is_pinned())\n",
    "    print(sample.tgt.is_pinned())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
